{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.chdir('..')\n",
    "from torch_geometric.loader import DataLoader\n",
    "from gnn_library.util import train, save, load\n",
    "from evaluate import evaluate_model, pp_output\n",
    "import instance_generator as ig\n",
    "import torch_converter as tc\n",
    "import evaluate as ev\n",
    "import osmnx as ox\n",
    "from util import Dataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch has version 1.12.0+cu102\n",
      "Using device: cuda:2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"PyTorch has version {}\".format(torch.__version__))\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    args = {\n",
    "        'processor':         'GENConv',\n",
    "        'head':              'regression',\n",
    "        'num_layers':        trial.suggest_int(\"num_layers{}\", 1, 6),\n",
    "        'num_mlp_layers':    trial.suggest_int(\"num_mlp_layers{}\", 1, 5), # TODO set to larger\n",
    "        'aggr':              'max',\n",
    "        'batch_size':        2**trial.suggest_int(\"log_batch_size\", 1, 6), \n",
    "        'node_feature_dim':  5,\n",
    "        'edge_feature_dim':  1,\n",
    "        'graph_feature_dim': 2,\n",
    "        'hidden_dim':        2**trial.suggest_int(\"hidden_dim\", 1, 7), # TODO set to 128\n",
    "        'output_dim':        1,\n",
    "        'dropout':           trial.suggest_float(\"dropout\", 0, 0.5),\n",
    "        'epochs':            2**trial.suggest_int(\"epochs\", 2, 8), # TODO set to larger\n",
    "        'opt':               trial.suggest_categorical(\"optimizer\", [\"adam\", \"adagrad\"]),\n",
    "        'opt_scheduler':     'none',\n",
    "        'opt_restart':       0,\n",
    "        'weight_decay':      5e-3, # TODO possibly modify\n",
    "        'lr':                trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True),\n",
    "        'device':            device,\n",
    "        'noise':             0\n",
    "    }\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\targs = define_model(trial)\n",
    "\ttrain_num = 200; test_num = 100\n",
    "\n",
    "\ter_config = {\n",
    "\t'graph_type': 'ER',\n",
    "\t'p': 0.75,\n",
    "\t'weighted': True\n",
    "\t}\n",
    "\tba_config = {\n",
    "\t'graph_type': 'BA',\n",
    "\t'ba_param': 4,\n",
    "\t'weighted': True\n",
    "\t}\n",
    "\tgeom_config = {\n",
    "\t\t'graph_type': 'GEOM',\n",
    "\t\t'q': 0.15,\n",
    "\t\t'd': 2,\n",
    "\t\t'weighted': True\n",
    "\t}\n",
    "\n",
    "\trng = np.random.default_rng()\n",
    "\n",
    "\n",
    "\ttrain_instances = [\n",
    "\t\t*ig.sample_instances(10, 6, train_num, rng, args, **er_config),\n",
    "\t\t*ig.sample_instances(10, 6, train_num, rng, args, **ba_config),\n",
    "\t\t*ig.sample_instances(10, 6, train_num, rng, args, **geom_config),\n",
    "\t]\n",
    "\n",
    "\ttest_instances = [\n",
    "\t\t*ig.sample_instances(10, 6, test_num, rng, args, **er_config),\n",
    "\t\t*ig.sample_instances(10, 6, test_num, rng, args, **ba_config),\n",
    "\t\t*ig.sample_instances(10, 6, test_num, rng, args, **geom_config),\n",
    "\t]\n",
    "\n",
    "\n",
    "\ttrain_data = Dataset(tc._instances_to_train_samples(train_instances, args['head']))\n",
    "\ttest_data = Dataset(tc._instances_to_train_samples(test_instances, args['head']))\n",
    "\n",
    "\ttrain_loader = DataLoader(\n",
    "\ttrain_data,\n",
    "\tbatch_size=args['batch_size'],\n",
    "\tshuffle=True,\n",
    "\tnum_workers=4\n",
    "\t)\n",
    "\n",
    "\ttest_loader = DataLoader(\n",
    "\ttest_data,\n",
    "\tbatch_size=args['batch_size'],\n",
    "\tshuffle=True,\n",
    "\tnum_workers=4\n",
    "\t)\n",
    "\n",
    "\t### Training\n",
    "\n",
    "\t_, _, test_accuracies, _, _ = train(train_loader, test_loader, args, trial)\n",
    "\tmodel_accuracy = test_accuracies[-1]\n",
    "\t\n",
    "\treturn model_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-25 12:13:32,352] Using an existing study with name 'hyperparam-study' instead of creating a new one.\n",
      "Training:   0%|          | 0/128 [00:00<?, ?Epochs/s]/home/alexhay/.local/lib/python3.10/site-packages/torch_geometric/utils/scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n",
      "Training: 100%|██████████| 128/128 [19:25<00:00,  9.10s/Epochs]\n",
      "[I 2024-01-25 12:33:12,285] Trial 5 finished with value: 0.19778189063072205 and parameters: {'num_layers{}': 4, 'num_mlp_layers{}': 4, 'log_batch_size': 4, 'hidden_dim': 3, 'dropout': 0.4558675989687025, 'epochs': 7, 'optimizer': 'adagrad', 'lr': 3.2632239763514146e-05}. Best is trial 5 with value: 0.19778189063072205.\n",
      "Training: 100%|██████████| 8/8 [02:05<00:00, 15.63s/Epochs]\n",
      "[I 2024-01-25 12:35:30,410] Trial 6 finished with value: 0.6293958425521851 and parameters: {'num_layers{}': 3, 'num_mlp_layers{}': 4, 'log_batch_size': 3, 'hidden_dim': 7, 'dropout': 0.46123485204481285, 'epochs': 3, 'optimizer': 'adagrad', 'lr': 3.220784369866894e-05}. Best is trial 6 with value: 0.6293958425521851.\n",
      "Training: 100%|██████████| 32/32 [01:23<00:00,  2.60s/Epochs]\n",
      "[I 2024-01-25 12:37:06,279] Trial 7 finished with value: 0.2203780561685562 and parameters: {'num_layers{}': 3, 'num_mlp_layers{}': 4, 'log_batch_size': 6, 'hidden_dim': 1, 'dropout': 0.391095639129533, 'epochs': 5, 'optimizer': 'adagrad', 'lr': 1.1685081321567396e-05}. Best is trial 6 with value: 0.6293958425521851.\n",
      "Training: 100%|██████████| 256/256 [28:49<00:00,  6.76s/Epochs]\n",
      "[I 2024-01-25 13:06:08,819] Trial 8 finished with value: 0.7888990044593811 and parameters: {'num_layers{}': 1, 'num_mlp_layers{}': 3, 'log_batch_size': 4, 'hidden_dim': 4, 'dropout': 0.12640939837120818, 'epochs': 8, 'optimizer': 'adam', 'lr': 0.04411753321121466}. Best is trial 8 with value: 0.7888990044593811.\n",
      "Training: 100%|██████████| 8/8 [02:42<00:00, 20.34s/Epochs]\n",
      "[I 2024-01-25 13:09:04,513] Trial 9 finished with value: 0.30244123935699463 and parameters: {'num_layers{}': 6, 'num_mlp_layers{}': 4, 'log_batch_size': 3, 'hidden_dim': 3, 'dropout': 0.443117635765002, 'epochs': 3, 'optimizer': 'adam', 'lr': 1.6518292019830694e-05}. Best is trial 8 with value: 0.7888990044593811.\n",
      "Training:   0%|          | 0/8 [00:31<?, ?Epochs/s]\n",
      "[I 2024-01-25 13:09:49,405] Trial 10 pruned. \n",
      "Training: 100%|██████████| 16/16 [19:25<00:00, 72.82s/Epochs]\n",
      "[I 2024-01-25 13:29:27,970] Trial 11 finished with value: 0.8523187637329102 and parameters: {'num_layers{}': 5, 'num_mlp_layers{}': 4, 'log_batch_size': 1, 'hidden_dim': 6, 'dropout': 0.15572192653175448, 'epochs': 4, 'optimizer': 'adam', 'lr': 3.657097702244298e-05}. Best is trial 11 with value: 0.8523187637329102.\n",
      "Training:   0%|          | 0/64 [01:14<?, ?Epochs/s]\n",
      "[I 2024-01-25 13:30:57,442] Trial 12 pruned. \n",
      "Training: 100%|██████████| 16/16 [16:27<00:00, 61.75s/Epochs]\n",
      "[I 2024-01-25 13:47:39,162] Trial 13 finished with value: 0.8039836883544922 and parameters: {'num_layers{}': 3, 'num_mlp_layers{}': 5, 'log_batch_size': 1, 'hidden_dim': 7, 'dropout': 0.2166712236864059, 'epochs': 4, 'optimizer': 'adagrad', 'lr': 0.00014352509208857156}. Best is trial 11 with value: 0.8523187637329102.\n",
      "Training: 100%|██████████| 8/8 [01:55<00:00, 14.49s/Epochs]\n",
      "[I 2024-01-25 13:49:47,375] Trial 14 finished with value: 0.7879495620727539 and parameters: {'num_layers{}': 2, 'num_mlp_layers{}': 3, 'log_batch_size': 3, 'hidden_dim': 3, 'dropout': 0.2523281468071299, 'epochs': 3, 'optimizer': 'adagrad', 'lr': 0.0014209950193587763}. Best is trial 11 with value: 0.8523187637329102.\n",
      "Training: 100%|██████████| 32/32 [01:43<00:00,  3.25s/Epochs]\n",
      "[I 2024-01-25 13:51:43,345] Trial 15 finished with value: 0.8640732169151306 and parameters: {'num_layers{}': 6, 'num_mlp_layers{}': 1, 'log_batch_size': 6, 'hidden_dim': 6, 'dropout': 0.0741381955782186, 'epochs': 5, 'optimizer': 'adam', 'lr': 0.0008671656879421813}. Best is trial 15 with value: 0.8640732169151306.\n",
      "Training: 100%|██████████| 32/32 [01:44<00:00,  3.27s/Epochs]\n",
      "[I 2024-01-25 13:53:39,740] Trial 16 finished with value: 0.8805704116821289 and parameters: {'num_layers{}': 6, 'num_mlp_layers{}': 1, 'log_batch_size': 6, 'hidden_dim': 6, 'dropout': 0.06808143618319495, 'epochs': 5, 'optimizer': 'adam', 'lr': 0.0008230489094408673}. Best is trial 16 with value: 0.8805704116821289.\n",
      "Training: 100%|██████████| 64/64 [03:29<00:00,  3.27s/Epochs]\n",
      "[I 2024-01-25 13:57:21,206] Trial 17 finished with value: 0.7472273707389832 and parameters: {'num_layers{}': 6, 'num_mlp_layers{}': 1, 'log_batch_size': 6, 'hidden_dim': 5, 'dropout': 0.022792915701751015, 'epochs': 6, 'optimizer': 'adam', 'lr': 0.0014689662767208068}. Best is trial 16 with value: 0.8805704116821289.\n",
      "Training:   0%|          | 0/32 [00:07<?, ?Epochs/s]\n",
      "[I 2024-01-25 13:57:40,407] Trial 18 pruned. \n",
      "Training:   0%|          | 0/64 [00:07<?, ?Epochs/s]\n",
      "[I 2024-01-25 13:57:59,247] Trial 19 pruned. \n",
      "Training:   0%|          | 0/4 [00:07<?, ?Epochs/s]\n",
      "[I 2024-01-25 13:58:18,844] Trial 20 pruned. \n",
      "Training: 100%|██████████| 128/128 [06:05<00:00,  2.85s/Epochs]\n",
      "[I 2024-01-25 14:04:35,432] Trial 21 finished with value: 0.8260669708251953 and parameters: {'num_layers{}': 4, 'num_mlp_layers{}': 2, 'log_batch_size': 6, 'hidden_dim': 7, 'dropout': 0.17497275392631903, 'epochs': 7, 'optimizer': 'adam', 'lr': 0.0002885018966453426}. Best is trial 16 with value: 0.8805704116821289.\n",
      "Training:   0%|          | 0/16 [00:06<?, ?Epochs/s]\n",
      "[I 2024-01-25 14:04:55,005] Trial 22 pruned. \n",
      "Training:  75%|███████▌  | 24/32 [01:29<00:29,  3.73s/Epochs]\n",
      "[I 2024-01-25 14:06:36,538] Trial 23 pruned. \n",
      "Training:   0%|          | 0/32 [00:11<?, ?Epochs/s]\n",
      "[I 2024-01-25 14:07:00,776] Trial 24 pruned. \n",
      "Training:   0%|          | 0/128 [00:04<?, ?Epochs/s]\n",
      "[I 2024-01-25 14:07:18,179] Trial 25 pruned. \n",
      "Training:   0%|          | 0/16 [00:45<?, ?Epochs/s]\n",
      "[I 2024-01-25 14:08:15,106] Trial 26 pruned. \n",
      "Training: 100%|██████████| 16/16 [09:43<00:00, 36.44s/Epochs]\n",
      "[I 2024-01-25 14:18:09,571] Trial 27 finished with value: 0.8551974892616272 and parameters: {'num_layers{}': 5, 'num_mlp_layers{}': 5, 'log_batch_size': 2, 'hidden_dim': 6, 'dropout': 0.04751841294365697, 'epochs': 4, 'optimizer': 'adam', 'lr': 0.0005790770694003612}. Best is trial 16 with value: 0.8805704116821289.\n",
      "Training: 100%|██████████| 64/64 [43:04<00:00, 40.38s/Epochs]\n",
      "[I 2024-01-25 15:01:26,521] Trial 28 finished with value: 0.5600000023841858 and parameters: {'num_layers{}': 6, 'num_mlp_layers{}': 5, 'log_batch_size': 2, 'hidden_dim': 7, 'dropout': 0.05145031764984753, 'epochs': 6, 'optimizer': 'adam', 'lr': 0.0004207333195906073}. Best is trial 16 with value: 0.8805704116821289.\n",
      "Training: 100%|██████████| 16/16 [01:25<00:00,  5.37s/Epochs]\n",
      "[I 2024-01-25 15:03:05,501] Trial 29 finished with value: 0.8639053106307983 and parameters: {'num_layers{}': 5, 'num_mlp_layers{}': 1, 'log_batch_size': 5, 'hidden_dim': 6, 'dropout': 0.03637895237128713, 'epochs': 4, 'optimizer': 'adam', 'lr': 0.000789546616592141}. Best is trial 16 with value: 0.8805704116821289.\n",
      "Training:   0%|          | 0/32 [00:07<?, ?Epochs/s]\n",
      "[I 2024-01-25 15:03:25,815] Trial 30 pruned. \n",
      "Training: 100%|██████████| 4/4 [00:13<00:00,  3.46s/Epochs]\n",
      "[I 2024-01-25 15:03:52,460] Trial 31 finished with value: 0.7876185774803162 and parameters: {'num_layers{}': 5, 'num_mlp_layers{}': 1, 'log_batch_size': 6, 'hidden_dim': 7, 'dropout': 0.021807467332421206, 'epochs': 2, 'optimizer': 'adam', 'lr': 0.00012689341920853704}. Best is trial 16 with value: 0.8805704116821289.\n",
      "Training:   0%|          | 0/16 [00:06<?, ?Epochs/s]\n",
      "[I 2024-01-25 15:04:12,354] Trial 32 pruned. \n",
      "Training:   0%|          | 0/64 [00:04<?, ?Epochs/s]\n",
      "[I 2024-01-25 15:04:30,089] Trial 33 pruned. \n",
      "Training: 100%|██████████| 32/32 [05:16<00:00,  9.88s/Epochs]\n",
      "[I 2024-01-25 15:09:58,250] Trial 34 finished with value: 0.8352614641189575 and parameters: {'num_layers{}': 5, 'num_mlp_layers{}': 2, 'log_batch_size': 4, 'hidden_dim': 5, 'dropout': 0.006291439881764102, 'epochs': 5, 'optimizer': 'adam', 'lr': 0.0009701222442504052}. Best is trial 16 with value: 0.8805704116821289.\n",
      "Training: 100%|██████████| 32/32 [02:35<00:00,  4.86s/Epochs]\n",
      "[I 2024-01-25 15:12:46,523] Trial 35 finished with value: 0.8328766822814941 and parameters: {'num_layers{}': 4, 'num_mlp_layers{}': 1, 'log_batch_size': 5, 'hidden_dim': 4, 'dropout': 0.04331480017940892, 'epochs': 5, 'optimizer': 'adam', 'lr': 0.0030169690576181073}. Best is trial 16 with value: 0.8805704116821289.\n",
      "Training:  25%|██▌       | 4/16 [00:52<02:36, 13.05s/Epochs]\n",
      "[I 2024-01-25 15:13:50,083] Trial 36 pruned. \n",
      "Training:  50%|█████     | 4/8 [00:18<00:18,  4.56s/Epochs]\n",
      "[I 2024-01-25 15:14:20,694] Trial 37 pruned. \n",
      "Training:  50%|█████     | 8/16 [05:33<05:33, 41.68s/Epochs]\n",
      "[I 2024-01-25 15:20:05,711] Trial 38 pruned. \n",
      "Training:   0%|          | 0/32 [00:04<?, ?Epochs/s]\n",
      "[I 2024-01-25 15:20:22,843] Trial 39 pruned. \n",
      "Training:   0%|          | 0/256 [00:19<?, ?Epochs/s]\n",
      "[I 2024-01-25 15:20:54,878] Trial 40 pruned. \n",
      "Training:  50%|█████     | 4/8 [00:28<00:28,  7.21s/Epochs]\n",
      "[I 2024-01-25 15:21:36,292] Trial 41 pruned. \n",
      "Training:  25%|██▌       | 4/16 [00:47<02:21, 11.80s/Epochs]\n",
      "[I 2024-01-25 15:22:36,117] Trial 42 pruned. \n",
      "Training:   0%|          | 0/8 [00:04<?, ?Epochs/s]\n",
      "[I 2024-01-25 15:22:52,875] Trial 43 pruned. \n",
      "Training: 100%|██████████| 32/32 [09:40<00:00, 18.14s/Epochs]\n",
      "[I 2024-01-25 15:32:45,159] Trial 44 finished with value: 0.8762035369873047 and parameters: {'num_layers{}': 5, 'num_mlp_layers{}': 2, 'log_batch_size': 3, 'hidden_dim': 6, 'dropout': 0.006127127559373079, 'epochs': 5, 'optimizer': 'adagrad', 'lr': 0.020140963784271132}. Best is trial 16 with value: 0.8805704116821289.\n",
      "Training: 100%|██████████| 64/64 [20:43<00:00, 19.43s/Epochs]\n",
      "[I 2024-01-25 15:53:39,461] Trial 45 finished with value: 0.8717135190963745 and parameters: {'num_layers{}': 6, 'num_mlp_layers{}': 2, 'log_batch_size': 3, 'hidden_dim': 3, 'dropout': 0.00851481975140432, 'epochs': 6, 'optimizer': 'adagrad', 'lr': 0.023040947310471332}. Best is trial 16 with value: 0.8805704116821289.\n",
      "Training:   0%|          | 0/64 [00:23<?, ?Epochs/s]\n",
      "[I 2024-01-25 15:54:16,558] Trial 46 pruned. \n",
      "Training:   3%|▎         | 4/128 [01:42<52:51, 25.57s/Epochs]\n",
      "[I 2024-01-25 15:56:15,433] Trial 47 pruned. \n",
      "Training:   0%|          | 0/64 [00:12<?, ?Epochs/s]\n",
      "[I 2024-01-25 15:56:41,645] Trial 48 pruned. \n",
      "Training:  25%|██▌       | 8/32 [02:45<08:17, 20.71s/Epochs]\n",
      "[I 2024-01-25 15:59:41,029] Trial 49 pruned. \n",
      "Training: 100%|██████████| 64/64 [15:05<00:00, 14.16s/Epochs]\n",
      "[I 2024-01-25 16:15:00,115] Trial 50 finished with value: 0.8629142045974731 and parameters: {'num_layers{}': 2, 'num_mlp_layers{}': 2, 'log_batch_size': 3, 'hidden_dim': 4, 'dropout': 0.0009000320243929896, 'epochs': 6, 'optimizer': 'adagrad', 'lr': 0.06356512228386672}. Best is trial 16 with value: 0.8805704116821289.\n",
      "Training:   0%|          | 0/32 [00:44<?, ?Epochs/s]\n",
      "[I 2024-01-25 16:15:57,258] Trial 51 pruned. \n",
      "Training:  12%|█▎        | 4/32 [00:51<05:58, 12.82s/Epochs]\n",
      "[I 2024-01-25 16:17:01,092] Trial 52 pruned. \n",
      "Training:   0%|          | 0/128 [00:05<?, ?Epochs/s]\n",
      "[I 2024-01-25 16:17:18,432] Trial 53 pruned. \n",
      "Training: 100%|██████████| 64/64 [17:52<00:00, 16.75s/Epochs]\n",
      "[I 2024-01-25 16:35:22,512] Trial 54 finished with value: 0.8852157592773438 and parameters: {'num_layers{}': 4, 'num_mlp_layers{}': 2, 'log_batch_size': 3, 'hidden_dim': 6, 'dropout': 0.0017141687474797355, 'epochs': 6, 'optimizer': 'adagrad', 'lr': 0.002583271978746284}. Best is trial 54 with value: 0.8852157592773438.\n",
      "Training: 100%|██████████| 64/64 [17:43<00:00, 16.61s/Epochs]\n",
      "[I 2024-01-25 16:53:17,655] Trial 55 finished with value: 0.8703024387359619 and parameters: {'num_layers{}': 4, 'num_mlp_layers{}': 2, 'log_batch_size': 3, 'hidden_dim': 4, 'dropout': 0.00931511346401253, 'epochs': 6, 'optimizer': 'adagrad', 'lr': 0.022045813193411756}. Best is trial 54 with value: 0.8852157592773438.\n",
      "Training:   0%|          | 0/64 [00:21<?, ?Epochs/s]\n",
      "[I 2024-01-25 16:53:51,923] Trial 56 pruned. \n",
      "Training:  38%|███▊      | 24/64 [07:14<12:03, 18.10s/Epochs]\n",
      "[I 2024-01-25 17:01:18,857] Trial 57 pruned. \n",
      "Training: 100%|██████████| 128/128 [36:10<00:00, 16.96s/Epochs]\n",
      "[I 2024-01-25 17:37:42,099] Trial 58 finished with value: 0.8767816424369812 and parameters: {'num_layers{}': 4, 'num_mlp_layers{}': 3, 'log_batch_size': 3, 'hidden_dim': 4, 'dropout': 0.0001644432306531253, 'epochs': 7, 'optimizer': 'adagrad', 'lr': 0.012294580876427071}. Best is trial 54 with value: 0.8852157592773438.\n",
      "Training: 100%|██████████| 256/256 [1:06:17<00:00, 15.54s/Epochs]\n",
      "[I 2024-01-25 18:44:12,252] Trial 59 finished with value: 0.8566849827766418 and parameters: {'num_layers{}': 3, 'num_mlp_layers{}': 3, 'log_batch_size': 3, 'hidden_dim': 4, 'dropout': 0.2547330629888698, 'epochs': 8, 'optimizer': 'adagrad', 'lr': 0.01332007717338873}. Best is trial 54 with value: 0.8852157592773438.\n",
      "Training:   0%|          | 0/128 [00:41<?, ?Epochs/s]\n",
      "[I 2024-01-25 18:45:05,063] Trial 60 pruned. \n",
      "Training:   3%|▎         | 4/128 [01:24<43:46, 21.18s/Epochs]\n",
      "[I 2024-01-25 18:46:41,874] Trial 61 pruned. \n",
      "Training:   0%|          | 0/256 [00:20<?, ?Epochs/s]\n",
      "[I 2024-01-25 18:47:15,178] Trial 62 pruned. \n",
      "Training:   0%|          | 0/128 [00:42<?, ?Epochs/s]\n",
      "[I 2024-01-25 18:48:09,264] Trial 63 pruned. \n",
      "Training: 100%|██████████| 64/64 [16:52<00:00, 15.83s/Epochs]\n",
      "[I 2024-01-25 19:05:15,367] Trial 64 finished with value: 0.8943089842796326 and parameters: {'num_layers{}': 3, 'num_mlp_layers{}': 3, 'log_batch_size': 3, 'hidden_dim': 5, 'dropout': 0.03064464576377201, 'epochs': 6, 'optimizer': 'adagrad', 'lr': 0.012190706489490367}. Best is trial 64 with value: 0.8943089842796326.\n",
      "Training:  12%|█▎        | 8/64 [01:14<08:38,  9.26s/Epochs]\n",
      "[I 2024-01-25 19:06:40,930] Trial 65 pruned. \n",
      "Training:  12%|█▎        | 8/64 [02:24<16:53, 18.10s/Epochs]\n",
      "[I 2024-01-25 19:09:18,206] Trial 66 pruned. \n",
      "Training: 100%|██████████| 64/64 [16:36<00:00, 15.57s/Epochs]\n",
      "[I 2024-01-25 19:26:06,674] Trial 67 finished with value: 0.8787466287612915 and parameters: {'num_layers{}': 3, 'num_mlp_layers{}': 2, 'log_batch_size': 3, 'hidden_dim': 5, 'dropout': 0.02868444793132912, 'epochs': 6, 'optimizer': 'adagrad', 'lr': 0.012513899859604183}. Best is trial 64 with value: 0.8943089842796326.\n",
      "Training: 100%|██████████| 32/32 [07:29<00:00, 14.03s/Epochs]\n",
      "[I 2024-01-25 19:33:47,271] Trial 68 finished with value: 0.8688221573829651 and parameters: {'num_layers{}': 2, 'num_mlp_layers{}': 3, 'log_batch_size': 3, 'hidden_dim': 5, 'dropout': 0.03675560055056426, 'epochs': 5, 'optimizer': 'adagrad', 'lr': 0.005243394825732821}. Best is trial 64 with value: 0.8943089842796326.\n",
      "Training: 100%|██████████| 128/128 [32:54<00:00, 15.42s/Epochs]\n",
      "[I 2024-01-25 20:06:52,894] Trial 69 finished with value: 0.8857938647270203 and parameters: {'num_layers{}': 3, 'num_mlp_layers{}': 2, 'log_batch_size': 3, 'hidden_dim': 6, 'dropout': 0.06469958823373965, 'epochs': 7, 'optimizer': 'adagrad', 'lr': 0.012117665347892279}. Best is trial 64 with value: 0.8943089842796326.\n",
      "Training:  19%|█▉        | 24/128 [12:32<54:21, 31.36s/Epochs]  \n",
      "[I 2024-01-25 20:19:36,840] Trial 70 pruned. \n",
      "Training:   0%|          | 0/128 [00:10<?, ?Epochs/s]\n",
      "[I 2024-01-25 20:19:59,892] Trial 71 pruned. \n",
      "Training: 100%|██████████| 256/256 [1:05:58<00:00, 15.46s/Epochs]\n",
      "[I 2024-01-25 21:26:09,974] Trial 72 finished with value: 0.8823529481887817 and parameters: {'num_layers{}': 3, 'num_mlp_layers{}': 2, 'log_batch_size': 3, 'hidden_dim': 6, 'dropout': 0.029881297126367976, 'epochs': 8, 'optimizer': 'adagrad', 'lr': 0.0022623528403559385}. Best is trial 64 with value: 0.8943089842796326.\n",
      "Training:   3%|▎         | 8/256 [01:54<59:13, 14.33s/Epochs]  \n",
      "[I 2024-01-25 21:28:15,278] Trial 73 pruned. \n",
      "Training:   0%|          | 0/256 [01:07<?, ?Epochs/s]\n",
      "[I 2024-01-25 21:29:34,950] Trial 74 pruned. \n",
      "Training: 100%|██████████| 256/256 [2:07:43<00:00, 29.93s/Epochs]  \n",
      "[I 2024-01-25 23:37:30,789] Trial 75 finished with value: 0.8776333332061768 and parameters: {'num_layers{}': 3, 'num_mlp_layers{}': 3, 'log_batch_size': 2, 'hidden_dim': 5, 'dropout': 0.071818999051151, 'epochs': 8, 'optimizer': 'adagrad', 'lr': 0.0024749614962230574}. Best is trial 64 with value: 0.8943089842796326.\n",
      "Training: 100%|██████████| 256/256 [2:08:18<00:00, 30.07s/Epochs]  \n",
      "[I 2024-01-26 01:46:01,807] Trial 76 finished with value: 0.8670914769172668 and parameters: {'num_layers{}': 3, 'num_mlp_layers{}': 3, 'log_batch_size': 2, 'hidden_dim': 5, 'dropout': 0.07413940939291626, 'epochs': 8, 'optimizer': 'adagrad', 'lr': 0.004681420017155671}. Best is trial 64 with value: 0.8943089842796326.\n",
      "Training:   0%|          | 0/256 [00:37<?, ?Epochs/s]\n",
      "[I 2024-01-26 01:46:51,943] Trial 77 pruned. \n",
      "Training: 100%|██████████| 128/128 [2:06:00<00:00, 59.06s/Epochs] \n",
      "[I 2024-01-26 03:53:03,689] Trial 78 finished with value: 0.8940305113792419 and parameters: {'num_layers{}': 3, 'num_mlp_layers{}': 3, 'log_batch_size': 1, 'hidden_dim': 6, 'dropout': 0.029374071860970186, 'epochs': 7, 'optimizer': 'adagrad', 'lr': 0.00269746627920032}. Best is trial 64 with value: 0.8943089842796326.\n",
      "Training:   3%|▎         | 8/256 [09:11<4:45:09, 68.99s/Epochs]\n",
      "[I 2024-01-26 04:02:28,413] Trial 79 pruned. \n",
      "Training: 100%|██████████| 256/256 [3:41:41<00:00, 51.96s/Epochs]  \n",
      "[I 2024-01-26 07:44:23,874] Trial 80 finished with value: 0.877993643283844 and parameters: {'num_layers{}': 2, 'num_mlp_layers{}': 2, 'log_batch_size': 1, 'hidden_dim': 6, 'dropout': 0.03334093374797002, 'epochs': 8, 'optimizer': 'adagrad', 'lr': 0.0022819387435361674}. Best is trial 64 with value: 0.8943089842796326.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  81\n",
      "  Number of pruned trials:  42\n",
      "  Number of complete trials:  34\n",
      "Best trial:\n",
      "  Value:  0.8943089842796326\n",
      "  Params: \n",
      "    num_layers{}: 3\n",
      "    num_mlp_layers{}: 3\n",
      "    log_batch_size: 3\n",
      "    hidden_dim: 5\n",
      "    dropout: 0.03064464576377201\n",
      "    epochs: 6\n",
      "    optimizer: adagrad\n",
      "    lr: 0.012190706489490367\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "study = optuna.create_study(study_name='hyperparam-study', direction='maximize', storage='sqlite:///hyperparam.db', load_if_exists=True)\n",
    "study.optimize(objective, n_trials=1000, timeout=60000)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "\tprint(\"    {}: {}\".format(key, value))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clrs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
