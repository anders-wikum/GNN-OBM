{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_generator as gg\n",
    "import numpy as np\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import NNConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from tqdm import trange\n",
    "import copy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "print(\"PyTorch has version {}\".format(torch.__version__))\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 8; n = 8; num = 10\n",
    "\n",
    "config = {\n",
    "    'graph_type': 'GEOM',\n",
    "    'threshold': 0.5,\n",
    "    'scaling': 1/ np.sqrt(2)\n",
    "}\n",
    "\n",
    "train_dataset = DataLoader(gg.generate_examples(num, m, n, [0.75] * m, **config), shuffle=True)\n",
    "test_dataset = DataLoader(gg.generate_examples(num, m, n, [0.75] * m, **config), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OBM_NNConv(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    GNN to predict node-level embeddings. Then applies a post-message passing layer to transform into the output\n",
    "    dimension.\n",
    "    Part of the code definition is inspired by Colab 2:\n",
    "    https://colab.research.google.com/drive/1xHmpjVO-Z74NK-dH3qoUBTf-tKUPfOKW?usp=sharing\n",
    "\n",
    "    The main model used for convolutions is NNConv from the \"Dynamic Edge-Conditioned Filters in Convolutional Neural\n",
    "    Networks on Graphs\" <https://arxiv.org/abs/1704.02901> paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, edge_feature_dim, args):\n",
    "        \"\"\"\n",
    "        Initializing the GNN\n",
    "        Args:\n",
    "            input_dim: dimension of node features\n",
    "            output_dim: output dimension required\n",
    "            edge_feature_dim: dimension of the edge features\n",
    "            args: object containing the rest of the GNN description, including the number of layers, dropout, ...\n",
    "        \"\"\"\n",
    "        super(OBM_NNConv, self).__init__()\n",
    "\n",
    "        hidden_dim = args.hidden_dim\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "        self.forward_conv = NNConv(input_dim, hidden_dim, nn.Linear(edge_feature_dim, input_dim * hidden_dim), aggr='max')\n",
    "        self.backward_conv = NNConv(hidden_dim, output_dim, nn.Linear(edge_feature_dim, output_dim * hidden_dim), aggr='max')\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.regression_head = torch.nn.Linear(output_dim, 1)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.forward_conv.reset_parameters()\n",
    "        self.backward_conv.reset_parameters()\n",
    "        self.batch_norm.reset_parameters()\n",
    "        self.regression_head.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.forward_conv(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, self.dropout, self.training)\n",
    "        x = self.backward_conv(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        return self.regression_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMSELoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, value_to_go, neighbor_mask):\n",
    "        \"\"\"\n",
    "        Computes MSE over neighbors of the arriving node.\n",
    "        Args:\n",
    "            pred: predicted node embeddings\n",
    "            value_to_go: array of underlying value to gos\n",
    "            neighbor_mask: mask for neighbors of arriving node\n",
    "\n",
    "        Returns:\n",
    "            Masked mean square error.\n",
    "        \"\"\"\n",
    "        \n",
    "        return F.mse_loss(pred[neighbor_mask].squeeze(dim=1), value_to_go)\n",
    "    \n",
    "\n",
    "def build_optimizer(args, params):\n",
    "    \"\"\"\n",
    "    Builds an optimizer according to the given parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p: p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr = args.lr, weight_decay = weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr = args.lr, momentum = 0.95, weight_decay = weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr = args.lr, weight_decay = weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr = args.lr, weight_decay = weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = args.opt_decay_step, gamma = args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = args.opt_restart)\n",
    "    return scheduler, optimizer\n",
    "\n",
    "\n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d\n",
    "\n",
    "# args defines the model and training hyperparameters\n",
    "args = {\n",
    "    'batch_size':    1,\n",
    "    'hidden_dim':    32,\n",
    "    'heads':         4,\n",
    "    'dropout':       0.5,\n",
    "    'epochs':        100,\n",
    "    'opt':           'adam',\n",
    "    'opt_scheduler': 'none',\n",
    "    'opt_restart':   0,\n",
    "    'weight_decay':  5e-3,\n",
    "    'lr':            0.001\n",
    "}\n",
    "args = objectview(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, test_model, loss_fn):\n",
    "    test_model.eval()\n",
    "    test_model.to(device)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = test_model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            loss = loss_fn(pred, batch.hint, batch.neighbors)\n",
    "            total_loss += loss * batch.num_graphs\n",
    "\n",
    "    total_loss /= len(loader.dataset)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader: DataLoader, test_loader: DataLoader, args: dict):\n",
    "    \"\"\"\n",
    "    Trains a GNN model, periodically testing it and accumulating loss values\n",
    "    Args:\n",
    "        args: dictionary object containing training parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # Input dimension is 1 (we only have demand information for every node)\n",
    "    # Edge feature dimension is 2 (capacity and cost per edge)\n",
    "    # Output dimension is 1 since we predict scalar potential values for each vertex\n",
    "    model = OBM_NNConv(4, 1, 1, args)\n",
    "    loss_fn = MaskedMSELoss()\n",
    "\n",
    "    _, opt = build_optimizer(args, model.parameters())\n",
    "    model.to(device)\n",
    "\n",
    "    # accumulate model performance for plotting\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_loss = None\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in trange(args.epochs, desc = \"Training\", unit = \"Epochs\"):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch.to(device)\n",
    "            opt.zero_grad()\n",
    "            pred = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            loss = loss_fn(pred, batch.hint, batch.neighbors)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        total_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(total_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            test_loss = test(test_loader, model, loss_fn)\n",
    "            test_losses.append(test_loss)\n",
    "            if best_loss is None or test_loss < best_loss:\n",
    "                best_loss = test_loss\n",
    "                best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "            test_losses.append(test_losses[-1])\n",
    "\n",
    "    return train_losses, test_losses, best_model, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_dataset, test_dataset, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clrs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
