{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_generator as gg\n",
    "import obm_dp as dp\n",
    "import numpy as np\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import NNConv, TransformerConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from util import diff\n",
    "from tqdm import trange\n",
    "import copy\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "print(\"PyTorch has version {}\".format(torch.__version__))\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10; n = 6; train_num = 100; test_num = 50\n",
    "\n",
    "er_config = {\n",
    "    'graph_type': 'ER',\n",
    "    'p': 1,\n",
    "    'weighted': True\n",
    "}\n",
    "ba_config = {\n",
    "    'graph_type': 'BA',\n",
    "    'ba_param': 4,\n",
    "    'weighted': True\n",
    "}\n",
    "geom_config = {\n",
    "    'graph_type': 'GEOM',\n",
    "    'threshold': 0.2,\n",
    "    'scaling': 1 / np.sqrt(2)\n",
    "}\n",
    "\n",
    "train_dataset = DataLoader(\n",
    "    [\n",
    "        *gg.generate_examples(train_num, m, n, [0.8] * m, **er_config),\n",
    "        *gg.generate_examples(train_num, m, n, [0.8] * m, **ba_config),\n",
    "        *gg.generate_examples(train_num, m, n, [0.8] * m, **geom_config)\n",
    "         \n",
    "    ]\n",
    ")\n",
    "test_dataset = DataLoader(\n",
    "    [\n",
    "        *gg.generate_examples(test_num, m, n, [0.8] * m, **er_config),\n",
    "        *gg.generate_examples(test_num, m, n, [0.8] * m, **ba_config),\n",
    "        *gg.generate_examples(test_num, m, n, [0.8] * m, **geom_config)\n",
    "         \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = list(test_dataset)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex.hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OBM_NNConv(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    GNN to predict node-level embeddings. Then applies a post-message passing layer to transform into the output\n",
    "    dimension.\n",
    "    Part of the code definition is inspired by Colab 2:\n",
    "    https://colab.research.google.com/drive/1xHmpjVO-Z74NK-dH3qoUBTf-tKUPfOKW?usp=sharing\n",
    "\n",
    "    The main model used for convolutions is NNConv from the \"Dynamic Edge-Conditioned Filters in Convolutional Neural\n",
    "    Networks on Graphs\" <https://arxiv.org/abs/1704.02901> paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, edge_feature_dim, args):\n",
    "        \"\"\"\n",
    "        Initializing the GNN\n",
    "        Args:\n",
    "            input_dim: dimension of node features\n",
    "            output_dim: output dimension required\n",
    "            edge_feature_dim: dimension of the edge features\n",
    "            args: object containing the rest of the GNN description, including the number of layers, dropout, ...\n",
    "        \"\"\"\n",
    "        super(OBM_NNConv, self).__init__()\n",
    "\n",
    "        hidden_dim = args.hidden_dim\n",
    "        self.dropout = args.dropout\n",
    "        aggr = args.aggr\n",
    "        self.num_layers = args.num_layers\n",
    "        conv_modules = [NNConv(input_dim, hidden_dim, nn.Linear(edge_feature_dim, input_dim * hidden_dim), aggr=aggr)]\n",
    "        conv_modules.extend(\n",
    "                [NNConv(hidden_dim, hidden_dim, nn.Linear(edge_feature_dim, hidden_dim * hidden_dim), aggr=aggr) for _ in\n",
    "                 range(self.num_layers - 1)])\n",
    "\n",
    "        self.convs = nn.ModuleList(conv_modules)\n",
    "        self.regression_head = nn.Linear(hidden_dim + 1, output_dim)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.regression_head.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, graph_features):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, self.training)\n",
    "        return self.regression_head(torch.hstack((x, graph_features.view(-1, 1))))\n",
    "    \n",
    "\n",
    "class OBM_TransformerConv(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    GNN to predict node-level embeddings. Then applies a post-message passing layer to transform into the output\n",
    "    dimension.\n",
    "    Part of the code definition is inspired by Colab 2:\n",
    "    https://colab.research.google.com/drive/1xHmpjVO-Z74NK-dH3qoUBTf-tKUPfOKW?usp=sharing\n",
    "\n",
    "    The main model used for convolutions is NNConv from the \"Dynamic Edge-Conditioned Filters in Convolutional Neural\n",
    "    Networks on Graphs\" <https://arxiv.org/abs/1704.02901> paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, edge_feature_dim, args):\n",
    "        \"\"\"\n",
    "        Initializing the GNN\n",
    "        Args:\n",
    "            input_dim: dimension of node features\n",
    "            output_dim: output dimension required\n",
    "            edge_feature_dim: dimension of the edge features\n",
    "            args: object containing the rest of the GNN description, including the number of layers, dropout, ...\n",
    "        \"\"\"\n",
    "        super(OBM_TransformerConv, self).__init__()\n",
    "\n",
    "        hidden_dim = args.hidden_dim\n",
    "        self.dropout = args.dropout\n",
    "        self.num_layers = args.num_layers\n",
    "        conv_modules = [TransformerConv(\n",
    "            in_channels=input_dim, \n",
    "            out_channels=hidden_dim,\n",
    "            heads=args.heads,\n",
    "            edge_dim=edge_feature_dim)]\n",
    "        conv_modules.extend(\n",
    "            [\n",
    "                TransformerConv(\n",
    "                    in_channels=hidden_dim * args.heads, \n",
    "                    out_channels=hidden_dim,\n",
    "                    heads=args.heads,\n",
    "                    edge_dim=edge_feature_dim\n",
    "                ) \n",
    "                for _ in range(self.num_layers - 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.convs = nn.ModuleList(conv_modules)\n",
    "        self.regression_head = nn.Linear(args.heads * hidden_dim + 1, output_dim)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.regression_head.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, graph_features):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, self.training)\n",
    "        return self.regression_head(torch.hstack((x, graph_features.view(-1, 1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMSELoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, value_to_go, neighbor_mask):\n",
    "        \"\"\"\n",
    "        Computes MSE over neighbors of the arriving node.\n",
    "        Args:\n",
    "            pred: predicted node embeddings\n",
    "            value_to_go: array of underlying value to gos\n",
    "            neighbor_mask: mask for neighbors of arriving node\n",
    "\n",
    "        Returns:\n",
    "            Masked mean square error.\n",
    "        \"\"\"\n",
    "        preds = pred[neighbor_mask].squeeze(dim=1)\n",
    "        return F.mse_loss(preds, value_to_go)\n",
    "    \n",
    "\n",
    "def build_optimizer(args, params):\n",
    "    \"\"\"\n",
    "    Builds an optimizer according to the given parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p: p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr = args.lr, weight_decay = weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr = args.lr, momentum = 0.95, weight_decay = weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr = args.lr, weight_decay = weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr = args.lr, weight_decay = weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = args.opt_decay_step, gamma = args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = args.opt_restart)\n",
    "    return scheduler, optimizer\n",
    "\n",
    "\n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, test_model, loss_fn):\n",
    "    test_model.eval()\n",
    "    test_model.to(device)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = test_model(batch.x, batch.edge_index, batch.edge_attr, batch.graph_features)\n",
    "            loss = loss_fn(pred, batch.hint, batch.neighbors)\n",
    "            total_loss += loss * batch.num_graphs\n",
    "\n",
    "    total_loss /= len(loader.dataset)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORKS = {\n",
    "    'TransformerConv': OBM_TransformerConv,\n",
    "    'NNConv': OBM_NNConv\n",
    "}\n",
    "\n",
    "def train(train_loader: DataLoader, test_loader: DataLoader, args: dict):\n",
    "    \"\"\"\n",
    "    Trains a GNN model, periodically testing it and accumulating loss values\n",
    "    Args:\n",
    "        args: dictionary object containing training parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # Input dimension is 1 (we only have demand information for every node)\n",
    "    # Edge feature dimension is 2 (capacity and cost per edge)\n",
    "    # Output dimension is 1 since we predict scalar potential values for each vertex\n",
    "    model = NETWORKS[args.processor](\n",
    "        args.node_feature_dim,\n",
    "        1,\n",
    "        args.edge_feature_dim,\n",
    "        args\n",
    "    )\n",
    "    loss_fn = MaskedMSELoss()\n",
    "\n",
    "    _, opt = build_optimizer(args, model.parameters())\n",
    "    model.to(device)\n",
    "\n",
    "    # accumulate model performance for plotting\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_loss = None\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in trange(args.epochs, desc = \"Training\", unit = \"Epochs\"):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch.to(device)\n",
    "            opt.zero_grad()\n",
    "            pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.graph_features)\n",
    "            loss = loss_fn(pred, batch.hint, batch.neighbors)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        total_loss /= len(train_loader.dataset)\n",
    "        print(total_loss)\n",
    "        train_losses.append(total_loss)\n",
    "\n",
    "        if epoch % 2 == 0:\n",
    "            test_loss = test(test_loader, model, loss_fn)\n",
    "            print(f'TEST LOSS: {test_loss}')\n",
    "            test_losses.append(test_loss)\n",
    "            if best_loss is None or test_loss < best_loss:\n",
    "                best_loss = test_loss\n",
    "                best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "            test_losses.append(test_losses[-1])\n",
    "\n",
    "    return train_losses, test_losses, best_model, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args defines the model and training hyperparameters\n",
    "args = {\n",
    "    'processor':     'NNConv',\n",
    "    'num_layers':    3,\n",
    "    'aggr':          'max',\n",
    "    'batch_size':    25,\n",
    "    'node_feature_dim': 4,\n",
    "    'edge_feature_dim': 1,\n",
    "    'hidden_dim':    128,\n",
    "    'heads':         4,\n",
    "    'dropout':       0.75,\n",
    "    'epochs':        50,\n",
    "    'opt':           'adam',\n",
    "    'opt_scheduler': 'none',\n",
    "    'opt_restart':   0,\n",
    "    'weight_decay':  5e-3,\n",
    "    'lr':            0.0001\n",
    "}\n",
    "args = objectview(args)\n",
    "\n",
    "_, _, trained_model, _ = train(train_dataset, test_dataset, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 250\n",
    "node_configs = [(48, 16), (32, 32), (16, 48)]\n",
    "graph_configs = [\n",
    "    {\n",
    "        'graph_type': 'ER',\n",
    "        'p': 0.75,\n",
    "        'weighted': True\n",
    "    },\n",
    "    {\n",
    "        'graph_type': 'BA',\n",
    "        'ba_param': 4,\n",
    "        'weighted': True\n",
    "    },\n",
    "    {\n",
    "        'graph_type': 'GEOM',\n",
    "        'threshold': 0.2,\n",
    "        'scaling': 1 / np.sqrt(2)\n",
    "    }\n",
    "]\n",
    "\n",
    "def test_model(trained_model, num_trials, node_configs, graph_configs):\n",
    "    for m, n in node_configs:\n",
    "        for config in graph_configs:\n",
    "            print(m, n, config)\n",
    "            greedy_vals = []\n",
    "            learned_vals = []\n",
    "            for _ in range(num_trials):\n",
    "                A = gg.sample_bipartite_graph(m, n, **config)\n",
    "                p = [0.8 for _ in range(m)]\n",
    "                coin_flips = [np.random.binomial(1, _p) for _p in p]\n",
    "                all_nodes = np.arange(n + m + 1)\n",
    "                offline_nodes = frozenset(np.arange(n))\n",
    "                matching = []\n",
    "                value = 0\n",
    "                for t in range(m):\n",
    "                    if coin_flips[t]:\n",
    "                        input = gg._to_pyg_test(A, p, offline_nodes, t)\n",
    "                        pred = trained_model(input.x, input.edge_index, input.edge_attr, input.graph_features)\n",
    "                        chosen_index = np.argmin(pred[input.neighbors].detach().numpy())\n",
    "                        choice = all_nodes[input.neighbors][chosen_index]\n",
    "                        if choice < n:\n",
    "                            matching.append((t, choice))\n",
    "                            value += A[t, choice]\n",
    "                    \n",
    "                        offline_nodes = diff(offline_nodes, choice)\n",
    "\n",
    "                for t in range(m):\n",
    "                    if not coin_flips[t]:\n",
    "                        A[t, :] = 0\n",
    "                row_ind, col_ind = linear_sum_assignment(A, maximize=True)\n",
    "                offline_opt = A[row_ind, col_ind].sum()\n",
    "                _, greed_value = dp.greedy(A, coin_flips)\n",
    "                learned_vals.append(value / offline_opt)\n",
    "                greedy_vals.append(greed_value / offline_opt)\n",
    "            \n",
    "            learned_mean = np.mean(learned_vals); greedy_mean = np.mean(greedy_vals)\n",
    "            learned_std = np.std(learned_vals, ddof=1); greedy_std = np.std(greedy_vals, ddof=1)\n",
    "            print(f\"Learned competitive ratio: {learned_mean} ± {2 * learned_std / np.sqrt(num_trials)}\")\n",
    "            print(f\"Greedy competitive ratio: {greedy_mean} ± {2 * greedy_std / np.sqrt(num_trials)}\")\n",
    "            print()\n",
    "\n",
    "test_model(trained_model, num_trials, node_configs, graph_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 250\n",
    "node_configs = [(24, 6), (28, 7), (10, 10), (11, 11), (20, 5), (24, 6)]\n",
    "graph_configs = [\n",
    "    {\n",
    "        'graph_type': 'ER',\n",
    "        'p': 0.75,\n",
    "        'weighted': True\n",
    "    },\n",
    "    {\n",
    "        'graph_type': 'BA',\n",
    "        'ba_param': 4,\n",
    "        'weighted': True\n",
    "    },\n",
    "    {\n",
    "        'graph_type': 'GEOM',\n",
    "        'threshold': 0.2,\n",
    "        'scaling': 1 / np.sqrt(2)\n",
    "    }\n",
    "]\n",
    "for config in graph_configs:    \n",
    "    for (m, n) in node_configs:\n",
    "        print(f\"online: {m}, offline: {n}, config: {config}\")\n",
    "        ratios = []\n",
    "        for _ in range(num_trials):\n",
    "            A = gg.sample_bipartite_graph(m, n, **config)\n",
    "            p = [0.8 for _ in range(m)]\n",
    "            coin_flips = [np.random.binomial(1, _p) for _p in p]\n",
    "            cache = dp.cache_stochastic_opt(A, p)\n",
    "            _, stoch_opt = dp.stochastic_opt(A, coin_flips, cache)\n",
    "            _, offline_opt = dp.offline_opt(A, coin_flips)\n",
    "            ratios.append(stoch_opt / offline_opt)\n",
    "        mean_ratio = np.mean(ratios);\n",
    "        std_ratio = np.std(ratios, ddof=1)\n",
    "        print(f\"Stoch opt competitive ratio: {mean_ratio} ± {2 * std_ratio / np.sqrt(num_trials)}\")\n",
    "        print()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10; n = 6; num_trials = 100\n",
    "\n",
    "config = {\n",
    "    'graph_type': 'ER',\n",
    "    'p': 0.75,\n",
    "    'weighted': True\n",
    "}\n",
    "def error_code(chosen_index, pred_index, length):\n",
    "    if chosen_index == pred_index and chosen_index == length - 1:\n",
    "        return \"Correct skip\"\n",
    "    elif chosen_index == pred_index:\n",
    "        return \"Correct no skip\"\n",
    "    elif chosen_index == length - 1:\n",
    "        return \"Incorrect no skip\"\n",
    "    elif pred_index == length - 1:\n",
    "        return \"Incorrect skip\"\n",
    "    else:\n",
    "        return \"Incorrect choice\"\n",
    "    \n",
    "record = ([[] for _ in range(m)], [[] for _ in range(m)], [[] for _ in range(m)])\n",
    "greedy_vals = []\n",
    "learned_vals = []\n",
    "for i in range(num_trials):\n",
    "    A = gg.sample_bipartite_graph(m, n, **config)\n",
    "    p = [1 for _ in range(m)]\n",
    "    cache = dp.cache_stochastic_opt(A, p)\n",
    "    coin_flips = [np.random.binomial(1, _p) for _p in p]\n",
    "    all_nodes = np.arange(n + m + 1)\n",
    "    offline_nodes = frozenset(np.arange(n))\n",
    "    matching = []\n",
    "    value = 0\n",
    "    for t in range(m):\n",
    "        if coin_flips[t]:\n",
    "            input = gg._to_pyg_test(A, p, offline_nodes, t)\n",
    "            pred = trained_model(input.x, input.edge_index, input.edge_attr)\n",
    "            \n",
    "            hints = dp.one_step_stochastic_opt(A, offline_nodes, t, cache)\n",
    "            hints = np.max(hints) - hints\n",
    "            print(t)\n",
    "            print(pred[input.neighbors].squeeze().detach().numpy())\n",
    "            print(hints)\n",
    "            print()\n",
    "            chosen_index = np.argmin(pred[input.neighbors].detach().numpy())\n",
    "            opt_index = np.argmin(hints)\n",
    "            choice = all_nodes[input.neighbors][chosen_index]\n",
    "            correct = (chosen_index == opt_index)\n",
    "            reduction = hints[opt_index] - hints[chosen_index]\n",
    "            record[0][t].append(error_code(opt_index, chosen_index, len(hints)))\n",
    "            record[1][t].append(reduction)\n",
    "            record[2][t].append(np.sum(input.neighbors.detach().numpy()))\n",
    "            if choice < n:\n",
    "                matching.append((t, choice))\n",
    "                value += A[t, choice]\n",
    "        \n",
    "            offline_nodes = diff(offline_nodes, choice)\n",
    "\n",
    "   \n",
    "    opt_matching, opt_value = dp.stochastic_opt(A, coin_flips, cache)\n",
    "    _, greed_value = dp.greedy(A, coin_flips)\n",
    "    learned_vals.append(value / opt_value)\n",
    "    greedy_vals.append(greed_value / opt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to predict skip more often?\n",
    "import matplotlib.pyplot as plt\n",
    "for t in range(m):\n",
    "  labels, vals =  np.unique(record[0][t], return_counts=True)\n",
    "  plt.bar(labels, vals)\n",
    "  plt.xticks(rotation=45)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(len(record[0])):\n",
    "    print(f\"Node: {i}, average reduction: {np.mean(record[1][i])}\")\n",
    "    print(f\"Node: {i}, average neighbors: {np.mean(record[2][i])}\")\n",
    "    plt.hist(record[1][i], bins=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value, opt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greed_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10; n = 6; num_trials = 1000\n",
    "\n",
    "# config = {\n",
    "#     'graph_type': 'ER',\n",
    "#     'p': 0.75,\n",
    "#     'weighted': True\n",
    "# }\n",
    "config = {\n",
    "    'graph_type': 'GEOM',\n",
    "    'threshold': 0.2,\n",
    "    'scaling': 1 / np.sqrt(2)\n",
    "}\n",
    "\n",
    "def _mask(offline_nodes, adj):\n",
    "    return [adj[i] for i in range(n) if i in offline_nodes and adj[i] > 0]\n",
    "\n",
    "\n",
    "record = ([[] for _ in range(m)], [[] for _ in range(m)], [[] for _ in range(m)])\n",
    "greedy_vals = []\n",
    "learned_vals = []\n",
    "for i in range(num_trials):\n",
    "    A = gg.sample_bipartite_graph(m, n, **config)\n",
    "    p = [1 for _ in range(m)]\n",
    "    cache = dp.cache_stochastic_opt(A, p)\n",
    "    coin_flips = [np.random.binomial(1, _p) for _p in p]\n",
    "    all_nodes = np.arange(n + m + 1)\n",
    "    offline_nodes = frozenset(np.arange(n))\n",
    "    matching = []\n",
    "    value = 0\n",
    "    for t in range(m):\n",
    "        if coin_flips[t]:\n",
    "            input = gg._to_pyg_test(A, p, offline_nodes, t)\n",
    "            pred = trained_model(input.x, input.edge_index, input.edge_attr) \\\n",
    "              [input.neighbors].detach().numpy()\n",
    "            masked_adj = _mask(offline_nodes, A[t, :])\n",
    "            hints = dp.one_step_stochastic_opt(A, offline_nodes, t, cache)\n",
    "            learned_index = np.argmax(pred)\n",
    "            try:\n",
    "              greedy_index = np.argmax(masked_adj)\n",
    "            except:\n",
    "              greedy_index = learned_index\n",
    "            \n",
    "            opt_index = np.argmax(hints)\n",
    "            if pred[learned_index] - pred[greedy_index] > 0.05:\n",
    "                chosen_index = learned_index\n",
    "            else:\n",
    "                chosen_index = greedy_index\n",
    "            \n",
    "            choice = all_nodes[input.neighbors][chosen_index]\n",
    "            correct = (chosen_index == opt_index)\n",
    "            reduction = hints[opt_index] - hints[chosen_index]\n",
    "            record[0][t].append(correct)\n",
    "            record[1][t].append(reduction)\n",
    "            record[2][t].append(np.sum(input.neighbors.detach().numpy()))\n",
    "            if choice < n:\n",
    "                matching.append((t, choice))\n",
    "                value += A[t, choice]\n",
    "        \n",
    "            offline_nodes = diff(offline_nodes, choice)\n",
    "\n",
    "   \n",
    "    opt_matching, opt_value = dp.stochastic_opt(A, coin_flips, cache)\n",
    "    _, greed_value = dp.greedy(A, coin_flips)\n",
    "    learned_vals.append(value / opt_value)\n",
    "    greedy_vals.append(greed_value / opt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_mean = np.mean(learned_vals); greedy_mean = np.mean(greedy_vals)\n",
    "learned_std = np.std(learned_vals, ddof=1); greedy_std = np.std(greedy_vals, ddof=1)\n",
    "print(f\"Learned competitive ratio: {learned_mean} ± {2 * learned_std / np.sqrt(num_trials)}\")\n",
    "print(f\"Greedy competitive ratio: {greedy_mean} ± {2 * greedy_std / np.sqrt(num_trials)}\")\n",
    "\n",
    "for i in range(len(record[0])):\n",
    "    print(f\"Node: {i}, proportion correct: {np.mean(record[0][i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(record[0])):\n",
    "    print(f\"Node: {i}, average reduction: {np.mean(record[1][i])}\")\n",
    "    print(f\"Node: {i}, average neighbors: {np.mean(record[2][i])}\")\n",
    "    plt.hist(record[1][i], bins=25)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clrs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
